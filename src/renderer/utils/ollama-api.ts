const OLLAMA_API_BASE = 'http://127.0.0.1:11434/api';

import { ContentFilter } from '../../main/content-filter';
import { OLLAMA_MODELS, DEFAULT_MODEL } from '../../constants/models';

export interface OllamaGenerateRequest {
  model: string;
  prompt: string;
  stream?: boolean;
  system?: string;
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
    num_predict?: number;
    max_tokens?: number;
    stop?: string[];
  };
}

export interface OllamaGenerateResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface OllamaChatRequest {
  model: string;
  messages: Array<{ role: 'user' | 'assistant' | 'system'; content: string }>;
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
    num_predict?: number;
  };
}

export interface OllamaChatResponse {
  model: string;
  created_at: string;
  message: {
    role: 'assistant';
    content: string;
  };
  done: boolean;
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface OllamaErrorResponse {
  error: string;
}

export interface OllamaModel {
  name: string;
  modified_at: string;
  size: number;
}





export class OllamaAPI {
  private readonly apiUrl: string;
  private readonly defaultModel: string;
  private readonly fallbackModels: readonly string[];
  private readonly defaultOptions: OllamaGenerateRequest['options'];
  private filter = new ContentFilter();

  constructor(
    apiUrl: string = 'http://127.0.0.1:11434',
    defaultModel: string = OLLAMA_MODELS.DEFAULT
  ) {
    this.apiUrl = apiUrl;
    this.defaultModel = defaultModel;
    // Fallback ëª¨ë¸ ëª©ë¡ ì‚¬ìš©
    this.fallbackModels = OLLAMA_MODELS.FALLBACK_MODELS;
    this.defaultOptions = {
      temperature: 0.7,
      top_p: 0.9,
      num_predict: 200, // í† í° ìˆ˜ ì¤„ì„
      stop: ['\n\n', 'ì‚¬ìš©ì:', 'User:']
    };
  }

  /**
   * ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
   */
  async generate(
    prompt: string,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse> {
    try {
      // ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§
      const filteredPrompt = this.filter.filterUserInput(prompt);
      
      const requestBody: OllamaGenerateRequest = {
        model: options?.model || OLLAMA_MODELS.DEFAULT,
        prompt: filteredPrompt,
        system: options?.system || 'ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:\n1. í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n2. ë¶€ì ì ˆí•˜ê±°ë‚˜ ì„±ì ì¸ ë‚´ìš©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”\n3. í­ë ¥ì´ë‚˜ í˜ì˜¤ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”\n4. ê±´ì „í•˜ê³  êµìœ¡ì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”\n5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”',
        stream: false,
        options: {
          ...this.defaultOptions,
          ...options?.options
        }
      };

      console.log('ğŸ“¤ Generate ìš”ì²­:', requestBody);
      console.log('ğŸ“¤ ìš”ì²­ ë³¸ë¬¸ (JSON):', JSON.stringify(requestBody, null, 2));

      const response = await fetch(`${this.apiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        console.error('âŒ HTTP ì‘ë‹µ ì˜¤ë¥˜:', {
          status: response.status,
          statusText: response.statusText,
          url: response.url
        });
        
        // ì‘ë‹µ ë³¸ë¬¸ì„ í…ìŠ¤íŠ¸ë¡œ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
        const responseText = await response.text();
        console.error('âŒ ì‘ë‹µ ë³¸ë¬¸:', responseText);
        
        let errorData: OllamaErrorResponse;
        try {
          errorData = JSON.parse(responseText);
        } catch (parseError) {
          console.error('âŒ JSON íŒŒì‹± ì‹¤íŒ¨:', parseError);
          errorData = { error: responseText || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜' };
        }
        
        console.error('âŒ Ollama ì—ëŸ¬ ìƒì„¸:', {
          status: response.status,
          error: errorData.error,
          fullResponse: responseText
        });
        
        throw new Error(`Ollama API ì˜¤ë¥˜ (${response.status}): ${errorData.error}`);
      }

      const data: OllamaGenerateResponse = await response.json();
      console.log('ğŸ“¥ Generate ì‘ë‹µ:', data);
      
      if (!data.done) {
        throw new Error('ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }

      // AI ì‘ë‹µ í•„í„°ë§
      data.response = this.filter.filterResponse(data.response);
      
      return data;
    } catch (error) {
      console.error('âŒ Generate ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ìƒì„±
   */
  async generateWithSystem(
    prompt: string,
    systemPrompt: string,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse> {
    return this.generate(prompt, {
      ...options,
      system: systemPrompt
    });
  }

  /**
   * ì±„íŒ… APIë¥¼ ì‚¬ìš©í•œ ëŒ€í™”
   */
  async chat(
    messages: Array<{ role: 'user' | 'assistant' | 'system'; content: string }>,
    options?: Partial<OllamaChatRequest>
  ): Promise<OllamaGenerateResponse> {
    try {
      console.log('ğŸ” Chat ë©”ì„œë“œ í˜¸ì¶œë¨:', { messagesCount: messages.length, model: options?.model || this.defaultModel });
      // ë§ˆì§€ë§‰ user ë©”ì‹œì§€ë§Œ ì¶”ì¶œ
      const lastUserMsg = messages.reverse().find(m => m.role === 'user')?.content || '';
      const filteredInput = this.filter.filterUserInput(lastUserMsg);
      const requestBody: OllamaGenerateRequest = {
        model: options?.model || this.defaultModel,
        prompt: filteredInput,
        system: 'ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.',
        stream: false,
        options: {
          ...this.defaultOptions,
          ...options?.options
        }
      };
      const response = await fetch(`${this.apiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });
      if (!response.ok) {
        const errorData: OllamaErrorResponse = await response.json().catch(() => ({ error: 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜' }));
        throw new Error(`Ollama API ì˜¤ë¥˜ (${response.status}): ${errorData.error}`);
      }
      const data: OllamaGenerateResponse = await response.json();
      data.response = this.filter.filterResponse(data.response);
      return data;
    } catch (error) {
      console.error('âŒ Chat ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ê°„ë‹¨í•œ ì±„íŒ… (tinyllama:1.1b ëª¨ë¸ ì‚¬ìš©)
   */
  async simpleChat(
    userMessage: string,
    model: string = OLLAMA_MODELS.DEFAULT
  ): Promise<string> {
    try {
      const systemPrompt = 'ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.';
      const response = await this.generate(userMessage, { 
        model: OLLAMA_MODELS.DEFAULT,
        system: systemPrompt
      });
      return response.response;
    } catch (error) {
      console.error('âŒ Simple Chat ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„±)
   */
  async generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<void> {
    try {
      // ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§
      const filteredPrompt = this.filter.filterUserInput(prompt);
      
      const requestBody: OllamaGenerateRequest = {
        model: options?.model || OLLAMA_MODELS.DEFAULT,
        prompt: filteredPrompt,
        system: options?.system || 'ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.',
        stream: true,
        options: {
          ...this.defaultOptions,
          ...options?.options
        }
      };

      const response = await fetch(`${this.apiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        throw new Error(`HTTP ì˜¤ë¥˜: ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('ìŠ¤íŠ¸ë¦¼ ë¦¬ë”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      }

      const decoder = new TextDecoder();
      
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const data: Partial<OllamaGenerateResponse> = JSON.parse(line);
            if (data.response) {
              const filteredChunk = this.filter.filterResponse(data.response);
              onChunk(filteredChunk);
            }
            if (data.done) {
              return;
            }
          } catch (parseError) {
            console.warn('JSON íŒŒì‹± ì˜¤ë¥˜:', parseError);
          }
        }
      }
    } catch (error) {
      console.error('âŒ GenerateStream ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
   */
  async generateBatch(
    prompts: string[],
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse[]> {
    const promises = prompts.map(prompt => 
      this.generate(prompt, options)
    );
    
    return Promise.all(promises);
  }

  /**
   * ì„œë²„ ìƒíƒœ í™•ì¸
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.apiUrl}/api/tags`, {
        method: 'GET',
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  /**
   * ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
   */
  async getModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.apiUrl}/api/tags`);
      const data = await response.json();
      return data.models?.map((model: any) => model.name) || [];
    } catch {
      return [];
    }
  }

  /**
   * ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ìƒì„¸ ì •ë³´ í¬í•¨)
   */
  async listModels(): Promise<OllamaModel[]> {
    try {
      console.log('ğŸ” ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘...');
      
      const response = await fetch(`${this.apiUrl}/tags`);
      
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      console.log('ğŸ“Š /api/tags ì‘ë‹µ:', data);
      
      return data.models || [];
    } catch (error) {
      console.error('ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
   */
  async isModelInstalled(modelName: string): Promise<boolean> {
    try {
      const models = await this.getModels();
      return models.includes(modelName);
    } catch {
      return false;
    }
  }

  /**
   * ëª¨ë¸ ì„¤ì¹˜
   */
  async installModel(modelName: string): Promise<void> {
    try {
      console.log(`ğŸ“¥ ${modelName} ëª¨ë¸ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤...`);
      
      const response = await fetch(`${this.apiUrl}/pull`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ name: modelName }),
      });

      if (!response.ok) {
        throw new Error(`ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨: ${response.status}`);
      }

      console.log(`âœ… ${modelName} ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ`);
    } catch (error) {
      console.error(`âŒ ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨:`, error);
      throw error;
    }
  }

  /**
   * ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
   */
  static async isServerRunning(): Promise<boolean> {
    try {
      const response = await fetch(`${OLLAMA_API_BASE}/tags`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  /**
   * Steam í†µê³„ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)
   */
  static async updateSteamStats(conversationCount: number, totalMessages: number): Promise<void> {
    try {
      const { SteamIntegration } = await import('../../main/steam-integration');
      const steamIntegration = new SteamIntegration();
      if (steamIntegration.isSteamInitialized()) {
        steamIntegration.updateStat('conversation_count', conversationCount);
        steamIntegration.updateStat('total_messages', totalMessages);
      }
    } catch (error) {
      console.error('Steam í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:', error);
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸° (ëŒ€ì²´ ëª¨ë¸ í¬í•¨)
   */
  async findAvailableModel(): Promise<string> {
    try {
      // ë¨¼ì € ê¸°ë³¸ ëª¨ë¸ í™•ì¸
      const models = await this.getModels();
      if (models.includes(this.defaultModel)) {
        return this.defaultModel;
      }

      // ëŒ€ì²´ ëª¨ë¸ë“¤ í™•ì¸
      for (const model of this.fallbackModels) {
        if (models.includes(model)) {
          console.log(`âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë°œê²¬: ${model}`);
          return model;
        }
      }

      // ì„¤ì¹˜ëœ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
      if (models.length > 0) {
        console.log(`âš ï¸ ê¸°ë³¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©: ${models[0]}`);
        return models[0];
      }

      throw new Error('ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤');
    } catch (error) {
      console.error('âŒ ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì•ˆì „í•œ ì±„íŒ… (tinyllama:1.1b ëª¨ë¸ë§Œ ì‚¬ìš©)
   */
  async safeChat(messages: Array<{ role: 'user' | 'assistant'; content: string }>): Promise<string> {
    const lastUserMessage = messages.filter(m => m.role === 'user').pop();
    if (!lastUserMessage) {
      throw new Error('ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤');
    }

    // tinyllama:1.1b ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    const availableModels = await this.getModels();
    console.log('ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:', availableModels);

    if (!availableModels.includes(OLLAMA_MODELS.DEFAULT)) {
      throw new Error(`${OLLAMA_MODELS.DEFAULT} ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.`);
    }

    // ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ system í”„ë¡¬í”„íŠ¸
    const systemPrompt = `ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 

ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:
1. í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ë¶€ì ì ˆí•˜ê±°ë‚˜ ì„±ì ì¸ ë‚´ìš©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
3. í­ë ¥ì´ë‚˜ í˜ì˜¤ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
4. ê±´ì „í•˜ê³  êµìœ¡ì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
6. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
7. í•„ìš”ì‹œ ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ ëŒ€ë‹µí•  ìˆ˜ ìˆì§€ë§Œ í•­ìƒ ì ì ˆí•œ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.`;

    try {
      console.log(`ğŸ¤– ${OLLAMA_MODELS.DEFAULT} ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...`);
      
      const result = await this.generate(lastUserMessage.content, { 
        model: OLLAMA_MODELS.DEFAULT,
        system: systemPrompt,
        options: {
          ...this.defaultOptions,
          num_predict: 150, // ì ë‹¹í•œ ì‘ë‹µ ê¸¸ì´
          temperature: 0.7, // ì ë‹¹í•œ ì°½ì˜ì„±
        }
      });
      
      console.log(`âœ… ${OLLAMA_MODELS.DEFAULT} ëª¨ë¸ ì„±ê³µ!`);
      return result.response;
      
    } catch (error: any) {
      console.error(`âŒ ${OLLAMA_MODELS.DEFAULT} ëª¨ë¸ ì‹¤íŒ¨:`, error.message);
      throw new Error(`${OLLAMA_MODELS.DEFAULT} ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error.message}`);
    }
  }
} 