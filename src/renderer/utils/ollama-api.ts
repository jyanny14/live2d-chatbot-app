const OLLAMA_API_BASE = 'http://127.0.0.1:11434/api';

export interface OllamaGenerateRequest {
  model: string;
  prompt: string;
  stream?: boolean;
  system?: string;
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
    num_predict?: number;
    max_tokens?: number;
    stop?: string[];
  };
}

export interface OllamaGenerateResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface OllamaChatRequest {
  model: string;
  messages: Array<{ role: 'user' | 'assistant' | 'system'; content: string }>;
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
    num_predict?: number;
  };
}

export interface OllamaChatResponse {
  model: string;
  created_at: string;
  message: {
    role: 'assistant';
    content: string;
  };
  done: boolean;
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface OllamaErrorResponse {
  error: string;
}

export interface OllamaModel {
  name: string;
  modified_at: string;
  size: number;
}

// Steam ì œì¶œì„ ìœ„í•œ ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
const SAFE_PROMPT_TEMPLATE = `
ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:

1. í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ë¶€ì ì ˆí•˜ê±°ë‚˜ ì„±ì ì¸ ë‚´ìš©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
3. í­ë ¥ì´ë‚˜ í˜ì˜¤ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
4. ê±´ì „í•˜ê³  êµìœ¡ì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

ì‚¬ìš©ì ì§ˆë¬¸: {prompt}
`;

export const OLLAMA_DEFAULT_MODEL = 'gemma:2b';

export class OllamaAPI {
  private readonly apiUrl: string;
  private readonly defaultModel: string;
  private readonly fallbackModels: string[];
  private readonly defaultOptions: OllamaGenerateRequest['options'];

  constructor(
    apiUrl: string = 'http://127.0.0.1:11434',
    defaultModel: string = 'tinyllama:1.1b'
  ) {
    this.apiUrl = apiUrl;
    this.defaultModel = defaultModel;
    // Windowsì—ì„œ ë” ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ëª¨ë¸ë“¤
    this.fallbackModels = [
      'tinyllama:1.1b',
      'llama2:7b',
      'llama2:7b-chat',
      'gemma:2b',
      'mistral:7b',
      'qwen2:0.5b'
    ];
    this.defaultOptions = {
      temperature: 0.7,
      top_p: 0.9,
      num_predict: 500,
      stop: ['\n\n', 'ì‚¬ìš©ì:', 'User:']
    };
  }

  /**
   * ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
   */
  async generate(
    prompt: string,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse> {
    try {
      // ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§
      const { ContentFilter } = await import('../../main/content-filter');
      const inputCheck = ContentFilter.filterUserInput(prompt);
      
      if (!inputCheck.isAppropriate) {
        throw new Error('ë¶€ì ì ˆí•œ ì…ë ¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
      }

      // ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
      const safePrompt = SAFE_PROMPT_TEMPLATE.replace('{prompt}', inputCheck.filteredInput);
      
      const requestBody: OllamaGenerateRequest = {
        model: options?.model || this.defaultModel,
        prompt: safePrompt,
        stream: false,
        options: {
          ...this.defaultOptions,
          ...options?.options
        },
        ...options
      };

      console.log('ğŸ“¤ Generate ìš”ì²­:', requestBody);

      const response = await fetch(`${this.apiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorData: OllamaErrorResponse = await response.json().catch(() => ({ error: 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜' }));
        throw new Error(`Ollama API ì˜¤ë¥˜ (${response.status}): ${errorData.error}`);
      }

      const data: OllamaGenerateResponse = await response.json();
      console.log('ğŸ“¥ Generate ì‘ë‹µ:', data);
      
      if (!data.done) {
        throw new Error('ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }

      // AI ì‘ë‹µ í•„í„°ë§
      data.response = ContentFilter.filterResponse(data.response);
      
      return data;
    } catch (error) {
      console.error('âŒ Generate ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ìƒì„±
   */
  async generateWithSystem(
    prompt: string,
    systemPrompt: string,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse> {
    return this.generate(prompt, {
      ...options,
      system: systemPrompt
    });
  }

  /**
   * ì±„íŒ… APIë¥¼ ì‚¬ìš©í•œ ëŒ€í™”
   */
  async chat(
    messages: Array<{ role: 'user' | 'assistant' | 'system'; content: string }>,
    options?: Partial<OllamaChatRequest>
  ): Promise<OllamaChatResponse> {
    try {
      console.log('ğŸ” Chat ë©”ì„œë“œ í˜¸ì¶œë¨:', { messagesCount: messages.length, model: options?.model || this.defaultModel });
      
      // ë©”ì‹œì§€ í•„í„°ë§
      const { ContentFilter } = await import('../../main/content-filter');
      
      const filteredMessages = messages.map(msg => {
        if (msg.role === 'user') {
          const inputCheck = ContentFilter.filterUserInput(msg.content);
          if (!inputCheck.isAppropriate) {
            throw new Error('ë¶€ì ì ˆí•œ ì…ë ¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
          }
          return { ...msg, content: inputCheck.filteredInput };
        }
        return { ...msg, content: ContentFilter.filterResponse(msg.content) };
      });

      const requestBody: OllamaChatRequest = {
        model: options?.model || this.defaultModel,
        messages: filteredMessages,
        stream: false,
        options: {
          ...this.defaultOptions,
          ...options?.options
        },
        ...options
      };

      console.log('ğŸ“¤ Chat ìš”ì²­:', requestBody);

      const response = await fetch(`${this.apiUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorData: OllamaErrorResponse = await response.json().catch(() => ({ error: 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜' }));
        throw new Error(`Ollama Chat API ì˜¤ë¥˜ (${response.status}): ${errorData.error}`);
      }

      const data: OllamaChatResponse = await response.json();
      console.log('ğŸ“¥ Chat ì‘ë‹µ:', data);
      
      if (!data.done) {
        throw new Error('ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }

      // AI ì‘ë‹µ í•„í„°ë§
      data.message.content = ContentFilter.filterResponse(data.message.content);
      
      return data;
    } catch (error) {
      console.error('âŒ Chat ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ê°„ë‹¨í•œ ì±„íŒ… (ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ë°›ì•„ì„œ ì²˜ë¦¬)
   */
  async simpleChat(
    userMessage: string,
    model: string = this.defaultModel
  ): Promise<string> {
    try {
      const response = await this.chat([
        { role: 'user', content: userMessage }
      ], { model });
      
      return response.message.content;
    } catch (error) {
      console.error('âŒ Simple Chat ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„±)
   */
  async generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<void> {
    try {
      // ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§
      const { ContentFilter } = await import('../../main/content-filter');
      const inputCheck = ContentFilter.filterUserInput(prompt);
      
      if (!inputCheck.isAppropriate) {
        throw new Error('ë¶€ì ì ˆí•œ ì…ë ¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
      }

      // ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
      const safePrompt = SAFE_PROMPT_TEMPLATE.replace('{prompt}', inputCheck.filteredInput);
      
      const requestBody: OllamaGenerateRequest = {
        model: options?.model || this.defaultModel,
        prompt: safePrompt,
        stream: true,
        options: {
          ...this.defaultOptions,
          ...options?.options
        },
        ...options
      };

      const response = await fetch(`${this.apiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        throw new Error(`HTTP ì˜¤ë¥˜: ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('ìŠ¤íŠ¸ë¦¼ ë¦¬ë”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      }

      const decoder = new TextDecoder();
      
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const data: Partial<OllamaGenerateResponse> = JSON.parse(line);
            if (data.response) {
              const filteredChunk = ContentFilter.filterResponse(data.response);
              onChunk(filteredChunk);
            }
            if (data.done) {
              return;
            }
          } catch (parseError) {
            console.warn('JSON íŒŒì‹± ì˜¤ë¥˜:', parseError);
          }
        }
      }
    } catch (error) {
      console.error('âŒ Stream ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
   */
  async generateBatch(
    prompts: string[],
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse[]> {
    const promises = prompts.map(prompt => 
      this.generate(prompt, options)
    );
    
    return Promise.all(promises);
  }

  /**
   * ì„œë²„ ìƒíƒœ í™•ì¸
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.apiUrl}/api/tags`, {
        method: 'GET',
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  /**
   * ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
   */
  async getModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.apiUrl}/api/tags`);
      const data = await response.json();
      return data.models?.map((model: any) => model.name) || [];
    } catch {
      return [];
    }
  }

  /**
   * ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ìƒì„¸ ì •ë³´ í¬í•¨)
   */
  async listModels(): Promise<OllamaModel[]> {
    try {
      console.log('ğŸ” ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘...');
      
      const response = await fetch(`${this.apiUrl}/tags`);
      
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      console.log('ğŸ“Š /api/tags ì‘ë‹µ:', data);
      
      return data.models || [];
    } catch (error) {
      console.error('ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
   */
  async isModelInstalled(modelName: string): Promise<boolean> {
    try {
      const models = await this.getModels();
      return models.includes(modelName);
    } catch {
      return false;
    }
  }

  /**
   * ëª¨ë¸ ì„¤ì¹˜
   */
  async installModel(modelName: string): Promise<void> {
    try {
      console.log(`ğŸ“¥ ${modelName} ëª¨ë¸ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤...`);
      
      const response = await fetch(`${this.apiUrl}/pull`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ name: modelName }),
      });

      if (!response.ok) {
        throw new Error(`ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨: ${response.status}`);
      }

      console.log(`âœ… ${modelName} ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ`);
    } catch (error) {
      console.error(`âŒ ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨:`, error);
      throw error;
    }
  }

  /**
   * ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
   */
  static async isServerRunning(): Promise<boolean> {
    try {
      const response = await fetch(`${OLLAMA_API_BASE}/tags`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  /**
   * Steam í†µê³„ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)
   */
  static async updateSteamStats(conversationCount: number, totalMessages: number): Promise<void> {
    try {
      const { SteamIntegration } = await import('../../main/steam-integration');
      const steamIntegration = new SteamIntegration();
      if (steamIntegration.isSteamInitialized()) {
        steamIntegration.updateStat('conversation_count', conversationCount);
        steamIntegration.updateStat('total_messages', totalMessages);
      }
    } catch (error) {
      console.error('Steam í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:', error);
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸° (ëŒ€ì²´ ëª¨ë¸ í¬í•¨)
   */
  async findAvailableModel(): Promise<string> {
    try {
      // ë¨¼ì € ê¸°ë³¸ ëª¨ë¸ í™•ì¸
      const models = await this.getModels();
      if (models.includes(this.defaultModel)) {
        return this.defaultModel;
      }

      // ëŒ€ì²´ ëª¨ë¸ë“¤ í™•ì¸
      for (const model of this.fallbackModels) {
        if (models.includes(model)) {
          console.log(`âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë°œê²¬: ${model}`);
          return model;
        }
      }

      // ì„¤ì¹˜ëœ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
      if (models.length > 0) {
        console.log(`âš ï¸ ê¸°ë³¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©: ${models[0]}`);
        return models[0];
      }

      throw new Error('ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤');
    } catch (error) {
      console.error('âŒ ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì•ˆì „í•œ ì±„íŒ… (ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ëª¨ë¸ ì‹œë„)
   */
  async safeChat(messages: Array<{ role: 'user' | 'assistant'; content: string }>): Promise<string> {
    const availableModel = await this.findAvailableModel();
    
    try {
      // ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
      const lastUserMessage = messages.filter(m => m.role === 'user').pop();
      if (!lastUserMessage) {
        throw new Error('ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤');
      }
      
      const result = await this.generate(lastUserMessage.content, { model: availableModel });
      return result.response;
    } catch (error) {
      console.error(`âŒ ëª¨ë¸ ${availableModel} ì‹¤íŒ¨:`, error);
      
      // ë‹¤ë¥¸ ëª¨ë¸ë“¤ ì‹œë„
      const models = await this.getModels();
      for (const model of models) {
        if (model !== availableModel) {
          try {
            console.log(`ğŸ”„ ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„: ${model}`);
            const lastUserMessage = messages.filter(m => m.role === 'user').pop();
            if (lastUserMessage) {
              const result = await this.generate(lastUserMessage.content, { model });
              return result.response;
            }
          } catch (retryError) {
            console.error(`âŒ ëª¨ë¸ ${model} ì‹¤íŒ¨:`, retryError);
          }
        }
      }
      
      throw new Error('ëª¨ë“  ëª¨ë¸ì—ì„œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤');
    }
  }
} 