import { ContentFilter } from '../../main/content-filter';
import { OLLAMA_MODELS, DEFAULT_MODEL } from '../../constants/models';
import { OllamaChatRequest, OllamaChatResponse, OllamaGenerateRequest, OllamaGenerateResponse } from '../../types';
import { generateUniqueId, truncateText, safeJsonParse } from '../../utils/common';

const OLLAMA_API_BASE = 'http://127.0.0.1:11434/api';

// ìƒˆë¡œìš´ Chat API ì¸í„°í˜ì´ìŠ¤ë“¤
export interface OllamaChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface OllamaErrorResponse {
  error: string;
}

export class OllamaAPI {
  private readonly apiUrl: string;
  private readonly defaultModel: string;
  private readonly defaultOptions: OllamaChatRequest['options'];
  // ContentFilterëŠ” ì •ì  ë©”ì„œë“œë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì¸ìŠ¤í„´ìŠ¤ ë¶ˆí•„ìš”
  private conversationHistory: OllamaChatMessage[] = [];
  private readonly maxHistoryLength = 20;

  constructor(
    apiUrl: string = 'http://127.0.0.1:11434',
    defaultModel: string = OLLAMA_MODELS.DEFAULT
  ) {
    this.apiUrl = apiUrl;
    this.defaultModel = defaultModel;
    this.defaultOptions = {
      temperature: 0.7,
      top_p: 0.9,
      num_predict: 200,
      num_thread: 4,
      num_ctx: 2048,
      stop: ['\n\n', 'ì‚¬ìš©ì:', 'User:']
    };
    
    // ë¹ˆ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¡œ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” Modelfileì—ì„œ ì²˜ë¦¬)
    this.conversationHistory = [];
  }

  /**
   * ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
   */
  private addMessage(role: 'user' | 'assistant', content: string): void {
    this.conversationHistory.push({ role, content });
    
    // ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)
    const nonSystemMessages = this.conversationHistory.filter(msg => msg.role !== 'system');
    
    if (nonSystemMessages.length > this.maxHistoryLength) {
      const systemMessages = this.conversationHistory.filter(msg => msg.role === 'system');
      const recentMessages = nonSystemMessages.slice(-this.maxHistoryLength);
      this.conversationHistory = [...systemMessages, ...recentMessages];
    }
  }

  /**
   * ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
   */
  clearHistory(): void {
    this.conversationHistory = [];
    console.log('ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”ë¨');
  }

  /**
   * í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
   */
  getHistory(): OllamaChatMessage[] {
    return [...this.conversationHistory];
  }

  /**
   * ëŒ€í™” íˆìŠ¤í† ë¦¬ ë””ë²„ê·¸ ì¶œë ¥
   */
  debugHistory(): void {
    console.log('ğŸ” í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬:');
    this.conversationHistory.forEach((msg, index) => {
      console.log(`[${index}] ${msg.role}: ${truncateText(msg.content, 150)}`);
    });
  }

  /**
   * Chat APIë¥¼ ì‚¬ìš©í•œ ëŒ€í™” (ìŠ¤íŠ¸ë¦¬ë°)
   */
  async chatStream(
    userMessage: string,
    onChunk: (chunk: string) => void,
    options?: Partial<OllamaChatRequest>
  ): Promise<string> {
    try {
      // ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§
      const filterResult = ContentFilter.filterUserInput(userMessage);
      const filteredMessage = filterResult.filteredInput;
      
      // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
      this.addMessage('user', filteredMessage);

      const requestBody: OllamaChatRequest = {
        model: options?.model || this.defaultModel,
        messages: this.conversationHistory,
        stream: true,
        options: {
          ...this.defaultOptions,
          ...options?.options
        }
      };

      console.log('ğŸ“¤ Chat ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­:', {
        model: requestBody.model,
        messageCount: requestBody.messages.length,
        stream: requestBody.stream
      });

      // ì‹¤ì œ ì „ì†¡ë˜ëŠ” ë©”ì‹œì§€ ë‚´ìš© í™•ì¸
      console.log('ğŸ“‹ ì „ì†¡ë˜ëŠ” ë©”ì‹œì§€ë“¤:', requestBody.messages.map((msg, index) => ({
        index,
        role: msg.role,
        content: truncateText(msg.content, 100)
      })));

      const response = await fetch(`${this.apiUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        const errorData = safeJsonParse<OllamaErrorResponse>(errorText, { error: 'Unknown error' });
        throw new Error(`Ollama API ì˜¤ë¥˜: ${errorData.error}`);
      }

      if (!response.body) {
        throw new Error('ì‘ë‹µ ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.');
      }

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';

      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) break;
          
          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split('\n').filter(line => line.trim());
          
          for (const line of lines) {
            try {
              const data: OllamaChatResponse = JSON.parse(line);
              
              if (data.message?.content) {
                const content = data.message.content;
                fullResponse += content;
                onChunk(content);
              }
              
              if (data.done) {
                break;
              }
            } catch (parseError) {
              console.warn('JSON íŒŒì‹± ì˜¤ë¥˜:', parseError, 'Line:', line);
            }
          }
        }
      } finally {
        reader.releaseLock();
      }

      // AI ì‘ë‹µ í•„í„°ë§
      const filteredResponse = ContentFilter.filterResponse(fullResponse);
      
      // ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
      this.addMessage('assistant', filteredResponse);
      
      console.log('ğŸ“¥ Chat ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ:', {
        responseLength: filteredResponse.length,
        totalMessages: this.conversationHistory.length
      });

      return filteredResponse;
    } catch (error) {
      console.error('âŒ Chat ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * Chat APIë¥¼ ì‚¬ìš©í•œ ëŒ€í™” (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
   */
  async chat(
    userMessage: string,
    options?: Partial<OllamaChatRequest>
  ): Promise<string> {
    try {
      // ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§
      const filterResult = ContentFilter.filterUserInput(userMessage);
      const filteredMessage = filterResult.filteredInput;
      
      // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
      this.addMessage('user', filteredMessage);

      const requestBody: OllamaChatRequest = {
        model: options?.model || this.defaultModel,
        messages: this.conversationHistory,
        stream: false,
        options: {
          ...this.defaultOptions,
          ...options?.options
        }
      };

      console.log('ğŸ“¤ Chat ìš”ì²­:', {
        model: requestBody.model,
        messageCount: requestBody.messages.length
      });

      const response = await fetch(`${this.apiUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        const errorData = safeJsonParse<OllamaErrorResponse>(errorText, { error: 'Unknown error' });
        throw new Error(`Ollama API ì˜¤ë¥˜: ${errorData.error}`);
      }

      const data: OllamaChatResponse = await response.json();
      
      if (!data.message?.content) {
        throw new Error('ì‘ë‹µì— ë©”ì‹œì§€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.');
      }

      // AI ì‘ë‹µ í•„í„°ë§
      const filteredResponse = ContentFilter.filterResponse(data.message.content);
      
      // ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
      this.addMessage('assistant', filteredResponse);
      
      console.log('ğŸ“¥ Chat ì™„ë£Œ:', {
        responseLength: filteredResponse.length,
        totalMessages: this.conversationHistory.length
      });

      return filteredResponse;
    } catch (error) {
      console.error('âŒ Chat ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * Generate APIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)
   */
  async generate(
    prompt: string,
    options?: Partial<OllamaGenerateRequest>
  ): Promise<OllamaGenerateResponse> {
    try {
      const requestBody: OllamaGenerateRequest = {
        model: options?.model || this.defaultModel,
        prompt,
        stream: false,
        system: options?.system,
        options: {
          temperature: 0.7,
          top_p: 0.9,
          num_predict: 200,
          ...options?.options
        }
      };

      console.log('ğŸ“¤ Generate ìš”ì²­:', {
        model: requestBody.model,
        promptLength: prompt.length
      });

      const response = await fetch(`${this.apiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        const errorData = safeJsonParse<OllamaErrorResponse>(errorText, { error: 'Unknown error' });
        throw new Error(`Ollama API ì˜¤ë¥˜: ${errorData.error}`);
      }

      const data: OllamaGenerateResponse = await response.json();
      
      console.log('ğŸ“¥ Generate ì™„ë£Œ:', {
        responseLength: data.response.length
      });

      return data;
    } catch (error) {
      console.error('âŒ Generate ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ì„œë²„ ìƒíƒœ í™•ì¸
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.apiUrl}/api/tags`, {
        method: 'GET',
        headers: {
          'Accept': 'application/json',
        },
      });
      
      return response.ok;
    } catch (error) {
      console.error('âŒ ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async getModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.apiUrl}/api/tags`, {
        method: 'GET',
        headers: {
          'Accept': 'application/json',
        },
      });

      if (!response.ok) {
        throw new Error(`ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: ${response.status}`);
      }

      const data = await response.json();
      return data.models?.map((model: any) => model.name) || [];
    } catch (error) {
      console.error('âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * íŠ¹ì • ëª¨ë¸ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
   */
  async isModelInstalled(modelName: string): Promise<boolean> {
    try {
      const models = await this.getModels();
      return models.includes(modelName);
    } catch (error) {
      console.error('âŒ ëª¨ë¸ ì„¤ì¹˜ í™•ì¸ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * ì •ì  ë©”ì„œë“œ: ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
   */
  static async isServerRunning(): Promise<boolean> {
    try {
      const response = await fetch(`${OLLAMA_API_BASE}/tags`, {
        method: 'GET',
        headers: {
          'Accept': 'application/json',
        },
      });
      
      return response.ok;
    } catch (error) {
      return false;
    }
  }

  /**
   * Steam í†µê³„ ì—…ë°ì´íŠ¸ (í–¥í›„ êµ¬í˜„)
   */
  static async updateSteamStats(conversationCount: number, totalMessages: number): Promise<void> {
    // TODO: Steam í†µê³„ ì—…ë°ì´íŠ¸ êµ¬í˜„
    console.log('ğŸ“Š Steam í†µê³„ ì—…ë°ì´íŠ¸:', { conversationCount, totalMessages });
  }
} 